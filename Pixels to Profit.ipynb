{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example for 9 stocks and 9 days case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional installations: \n",
    "#!pip install yfinance --upgrade --no-cache-dir\n",
    "#!pip3 install pandas_datareader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "from yahoofinancials import YahooFinancials\n",
    "import ta\n",
    "\n",
    "from math import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#optional \n",
    "yf.pdr_override() # <== that's all it takes :-)\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# for pandas_datareader, otherwise it might have issues, sometimes there is some version mismatch\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "\n",
    "# make pandas to print dataframes nicely\n",
    "pd.set_option('expand_frame_repr', False)  \n",
    "\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np\n",
    "from scipy.stats import norm, t\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import timedelta, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    # df on input should contain only one column with the price data (plus dataframe index)\n",
    "    min = df.min()\n",
    "    max = df.max()\n",
    "    x = df \n",
    "    \n",
    "    # time series normalization part\n",
    "    # y will be a column in a dataframe\n",
    "    y = (x-min) / (max-min)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "    # One can also use Min-Max Scaler from sklearn library and get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the datframes\n",
    "\n",
    "df_AAPL = pd.read_excel(\"AAPL.xlsx\")\n",
    "df_AMD = pd.read_excel('AMD.xlsx')\n",
    "df_AMZN = pd.read_excel(\"AMZN.xlsx\")\n",
    "df_IBM = pd.read_excel('IBM.xlsx')\n",
    "df_ORC = pd.read_excel('Oracle.xlsx')\n",
    "df_MSFT = pd.read_excel('MSFT.xlsx')\n",
    "df_INTC = pd.read_excel('INTC.xlsx')\n",
    "df_ATVI = pd.read_excel('ATVI.xlsx')\n",
    "df_NVDA = pd.read_excel('NVDA.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### check any dataframe has any extra row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe 3 has an extra row with Date ['2001-09-11T00:00:00.000000000']\n",
      "Dataframe 7 has an extra row with Date ['2001-09-11T00:00:00.000000000']\n",
      "Dataframe 9 has an extra row with Date ['2001-09-11T00:00:00.000000000']\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_AAPL, df_AMD, df_AMZN, df_IBM, df_ORC, df_MSFT, df_ATVI, df_INTC, df_NVDA]\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    if len(df) != len(df_AAPL):\n",
    "        extra_row = df[~df['Date'].isin(df_AAPL['Date'])]\n",
    "        print(f\"Dataframe {i+1} has an extra row with Date {extra_row['Date'].values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMZN & ATVI & NVDA has 09/11 so remove them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMZN = df_AMZN[df_AMZN['Date'] != '2001-09-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMZN = df_AMZN.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ATVI = df_ATVI[df_ATVI['Date'] != '2001-09-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ATVI = df_ATVI.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NVDA = df_NVDA[df_NVDA['Date'] != '2001-09-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NVDA = df_NVDA.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n"
     ]
    }
   ],
   "source": [
    "df_list = [df_AAPL, df_AMD, df_AMZN, df_IBM, df_ORC, df_ATVI, df_IBM, df_MSFT, df_NVDA]\n",
    "\n",
    "for df in df_list:\n",
    "    print(\"The number of rows in the DataFrame is:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate all the features and normalise those\n",
    "def do_all(df):\n",
    "    \n",
    "    df['gap'] = df['Open '] - df['Close'].shift(1)\n",
    "    df = df.dropna()\n",
    "    df['Close'] = normalize_data(df['Close'])\n",
    "    df['Volume'] = normalize_data(df['Volume'])\n",
    "    df['gap'] = normalize_data(df['gap'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAPL_Norm = do_all(df_AAPL) ##do for all the stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_AAPL_Norm, df_AMD_Norm, df_AMZN_Norm, df_IBM_Norm, df_ORC_Norm, df_MSFT_Norm, df_INTC_Norm, df_ATVI_Norm, df_NVDA_Norm]\n",
    "\n",
    "for df in df_list:\n",
    "    print(\"The number of rows in the DataFrame is:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you make the dataframe to array and resize it\n",
    "\n",
    "def prepare_for_stacking(name_df):\n",
    "    selected_columns = name_df[['gap', 'Close', 'Volume']]\n",
    "    arr = selected_columns.values\n",
    "    arr = arr.reshape(-1,9,3)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be one NaN drop that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_AAPL_Norm, df_AMD_Norm, df_AMZN_Norm, df_IBM_Norm, df_ORC_Norm, df_MSFT_Norm, df_INTC_Norm, df_ATVI_Norm, df_NVDA_Norm ]\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    df.drop(df.tail(1).index, inplace=True) ### Drop a couple of rows as 5086 isn't divisible by 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_AAPL_Norm, df_AMD_Norm, df_AMZN_Norm, df_IBM_Norm, df_ORC_Norm, df_MSFT_Norm, df_INTC_Norm, df_ATVI_Norm, df_NVDA_Norm]\n",
    "\n",
    "for df in df_list:\n",
    "    print(\"The number of rows in the DataFrame is:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSFT_arr = prepare_for_stacking(df_MSFT_Norm)\n",
    "\n",
    "AAPL_arr = prepare_for_stacking(df_AAPL_Norm)\n",
    "\n",
    "AMZN_arr = prepare_for_stacking(df_AMZN_Norm)\n",
    "\n",
    "AMD_arr = prepare_for_stacking(df_AMD_Norm)\n",
    "\n",
    "IBM_arr = prepare_for_stacking(df_IBM_Norm)\n",
    "\n",
    "ORC_arr = prepare_for_stacking(df_ORC_Norm)\n",
    "\n",
    "INTC_arr = prepare_for_stacking(df_INTC_Norm)\n",
    "\n",
    "ATVI_arr = prepare_for_stacking(df_ATVI_Norm)\n",
    "\n",
    "NVDA_arr = prepare_for_stacking(df_NVDA_Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the stocks according to beta order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list on the basis of beta values\n",
    "combined_array = [ATVI_arr, IBM_arr, INTC_arr, MSFT_arr, ORC_arr, AAPL_arr, NVDA_arr, AMZN_arr, AMD_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack those\n",
    "data_array = np.hstack(combined_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array = data_array.reshape((645, 9, 9, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(final_array[580], interpolation=\"nearest\", cmap=\"Pastel1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making pictures here in folder\n",
    "%%time\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "output_folder = r\"F:\\Spatio-temporal\\9cross9\"\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.mkdir(output_folder)\n",
    "\n",
    "# for i, arrays in enumerate(final_array):\n",
    "#     current_folder = os.path.join(output_folder, str(i))\n",
    "#     if not os.path.exists(current_folder):\n",
    "#         os.mkdir(current_folder)\n",
    "for index, array in enumerate(final_array):\n",
    "    #output_folder = f\"C:/Users/Asus/Spatio-Temporal Finance/My Implementations/10 cross 10/Images/00000/{index}\"\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(array, interpolation=\"nearest\", cmap=\"viridis\")\n",
    "    plt.axis('off') #remove axis\n",
    "    fig.savefig(Path(output_folder, f\"9cross9_close_{index}.jpg\"), bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVaR Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_AAPL[['Close']]\n",
    "df2 = df_AMZN[['Close']]\n",
    "df3 = df_AMD[['Close']]\n",
    "df4 = df_IBM[['Close']]\n",
    "df5 = df_ORC[['Close']]\n",
    "df6 = df_MSFT[['Close']]\n",
    "df7 = df_INTC[['Close']]\n",
    "df8 = df_ATVI[['Close']]\n",
    "df9 = df_NVDA[['Close']]\n",
    "\n",
    "df1.columns = ['AAPL']\n",
    "df2.columns = ['AMZN']\n",
    "df3.columns = ['AMD']\n",
    "df4.columns = ['IBM']\n",
    "df5.columns = ['ORC']\n",
    "df6.columns = ['MSFT']\n",
    "df7.columns = ['INTC']\n",
    "df8.columns = ['ATVI']\n",
    "df9.columns = ['NVDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n",
      "The number of rows in the DataFrame is: 5807\n"
     ]
    }
   ],
   "source": [
    "df_list = [df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "\n",
    "for df in df_list:\n",
    "    print(\"The number of rows in the DataFrame is:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialse the weights here equal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "       0.11111111, 0.11111111, 0.11111111, 0.11111111])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = ['AMZN', 'AAPL', 'IBM', 'ORC', 'AMD', 'MSFT', 'INTC', 'ATVI', 'NVDA']\n",
    "weights = np.array([1/len(stocks) for n in stocks])\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = new_df.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.drop(returns.tail(1).index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns['portfolio'] = returns.dot(weights) #create the portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff22 = returns[returns['portfolio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1date = df_AAPL[['Date']]\n",
    "df1date.drop(df1date.head(1).index, inplace=True)\n",
    "df1date.drop(df1date.tail(1).index, inplace=True)\n",
    "\n",
    "df1date.columns = ['Date'] #added the date column as index\n",
    "dff22.columns = ['portfolio'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvar = pd.concat([df1date, dff22], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvar.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VaR CVaR Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```VaR```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historicalVaR(returns, alpha=5):\n",
    "    \"\"\"\n",
    "    Read in a pandas dataframe of returns / a pandas series of returns.\n",
    "    Output the percentile of the distribution at the given alpha confidence level.\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        return np.percentile(returns, alpha)\n",
    "    \n",
    "    # A passed user-defined-function will be passed a Series for evaluation.\n",
    "    \n",
    "    elif isinstance(returns, pd.DataFrame):\n",
    "        return returns.aggregate(historicalVaR, alpha=alpha)\n",
    "   \n",
    "    else:\n",
    "        raise TypeError(\"Expected returns to be dataframe or series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```CVaR```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historicalCVaR(returns, alpha=5):\n",
    "    \"\"\"\n",
    "    Read in a pandas dataframe of returns / a pandas series of returns\n",
    "    Output the CVaR for dataframe / series\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        belowVaR = returns <= historicalVaR(returns, alpha=alpha)\n",
    "        return returns[belowVaR].mean()\n",
    "    \n",
    "    # A passed user-defined-function will be passed a Series for evaluation.\n",
    "    \n",
    "    elif isinstance(returns, pd.DataFrame):\n",
    "        return returns.aggregate(historicalCVaR, alpha=alpha)\n",
    "   \n",
    "    else:\n",
    "        raise TypeError(\"Expected returns to be dataframe or series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = cvar.index // 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvar_final = cvar.groupby(cvar.index // 9).apply(historicalCVaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the crash based on CVaR percentage\n",
    "tt = (cvar_final.loc[cvar_final['portfolio'] < -0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = list(tt.index) ##crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following CSV represents the distance matrix derived from feature vectors obtained using various pre-trained neural networks as specified in our paper (page no 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dff212 is resnet low, dff21 is resnet high and dff22 is Vit (using one at a time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff212 = pd.read_csv('9.csv')\n",
    "#dff21 = pd.read_csv('9 (1).csv')\n",
    "dff22 = pd.read_csv('9 (2).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff212.drop(['Unnamed: 0'], axis=1, inplace = True)\n",
    "#dff21.drop(['Unnamed: 0'], axis=1, inplace = True)\n",
    "dff22.drop(['Unnamed: 0'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**matrix0 is resnet low, matrix1 is resnet high and matrix2 is Vit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix0 = dff212.to_numpy()\n",
    "#matrix1 = dff21.to_numpy()\n",
    "matrix2 = dff22.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(matrix, event_list, iterations=1000):\n",
    "    \"\"\"\n",
    "    matrix: Mx2048 matrix, time series data\n",
    "    event_list: list of indices, indicates which rows belong to event class\n",
    "    iterations: number of iterations to perform the bootstrapping\n",
    "    \"\"\"\n",
    "    # Split the matrix into training and testing sets at the median date\n",
    "    mid = matrix.shape[0] // 2\n",
    "    training_data = matrix[:mid, :]\n",
    "    testing_data = matrix[mid:, :]\n",
    "    \n",
    "    event_index = [num for num in index_list if num < mid] #get all the event index from the cvar index list until mid\n",
    "    non_event_index = [num for num in range(0, mid) if num not in index_list] #all natural no not present in event index list until mid\n",
    "    \n",
    "    event_event_dist = []\n",
    "    event_non_event_dist = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Compute the event/event distribution\n",
    "        event_pairs = random.sample(event_index, k=len(event_index)) #length changed to full length\n",
    "        event_pair_scores = []\n",
    "        for i, j in zip(event_pairs[::2], event_pairs[1::2]): #this basically making pairs and then we'll calc man distance\n",
    "            score = np.sum(np.abs(training_data[i, :] - training_data[j, :]))  #manhattan\n",
    "            #score = distance.minkowski(training_data[i, :], training_data[j, :], 2)\n",
    "            event_pair_scores.append(score)\n",
    "        event_event_dist.append(np.mean(event_pair_scores))\n",
    "        \n",
    "        # Compute the event/non-event distribution\n",
    "        event_non_event_pairs = [(random.choice(event_index), random.choice(non_event_index)) for _ in range(len(event_index))]\n",
    "        event_non_event_pair_scores = []\n",
    "        for i, j in event_non_event_pairs:\n",
    "            score = np.sum(np.abs(training_data[i, :] - training_data[j, :]))   #manhattan\n",
    "            #score = distance.minkowski(training_data[i, :], training_data[j, :], 2)\n",
    "            event_non_event_pair_scores.append(score)\n",
    "        event_non_event_dist.append(np.mean(event_non_event_pair_scores))\n",
    "    \n",
    "    return event_event_dist, event_non_event_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do bootstrapping for vit\n",
    "pp_vit = bootstrap(matrix1, index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below all the _list_1 denotes the event_event_dist and _list_2 denotes event_nonevent_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_list1 = pp_vit[0]\n",
    "vit_list2 = pp_vit[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_list1 = pp_res[0]\n",
    "# res_list2 = pp_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A histogram to check normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the list\n",
    "\n",
    "# Plot the histogram using the list\n",
    "plt.hist(res_list1, bins=10, edgecolor='black')\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot the first histogram using the first list\n",
    "n, bins, patches = plt.hist(vit_list1, bins=10, color='red', alpha=0.5, density=True)\n",
    "\n",
    "# Plot the second histogram using the second list\n",
    "n, bins, patches = plt.hist(vit_list2, bins=10, color='blue', alpha=0.5, density=True)\n",
    "\n",
    "# Add transparency to the overlap between the two histograms\n",
    "for i in range(10):\n",
    "    patches[i].set_fc('grey')\n",
    "    patches[i].set_alpha(0.5)\n",
    "\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"9*9 bootstrapped distribution 4% event class ViT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import matplotlib.mlab as mlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best fit of data\n",
    "(mu1, sigma1) = norm.fit(pp_vit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Generate the data sets\n",
    "vit_list1 = pp_vit[0]\n",
    "vit_list2 = pp_vit[1]\n",
    "\n",
    "# Plot the histograms\n",
    "n, bins, patches = plt.hist(vit_list1, bins=10, color='red', alpha=0.5, density=True)\n",
    "n, bins, patches = plt.hist(vit_list2, bins=10, color='blue', alpha=0.5, density=True)\n",
    "\n",
    "# Calculate the mean and standard deviation of each data set\n",
    "mean1, std1 = norm.fit(vit_list1)\n",
    "mean2, std2 = norm.fit(vit_list2)\n",
    "\n",
    "# Generate the best-fit Gaussian curves\n",
    "x = np.linspace(min(bins), max(bins), 100)\n",
    "pdf1 = norm.pdf(x, mean1, std1)\n",
    "pdf2 = norm.pdf(x, mean2, std2)\n",
    "\n",
    "# Add the Gaussian curves to the plot\n",
    "plt.plot(x, pdf1, color='red', linestyle='--', label='Gaussian fit 1')\n",
    "plt.plot(x, pdf2, color='blue', linestyle='--', label='Gaussian fit 2')\n",
    "\n",
    "# Add transparency to the overlap between the two histograms\n",
    "for i in range(10):\n",
    "    patches[i].set_fc('grey')\n",
    "    patches[i].set_alpha(0.5)\n",
    "\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram with Gaussian fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing event_event & event_non_event mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_event_mean = np.mean(pp_res[0])\n",
    "event_non_event_mean = np.mean(pp_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_event_var = (np.std(pp_res[0]))**2\n",
    "event_non_event_var = (np.std(pp_res[1]))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob(new_observation, mean, variance):\n",
    "    # calculate the probability of the new observation belonging to the distribution\n",
    "    prob = stats.norm.pdf(new_observation, mean, variance**0.5)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(new_observation, event_event_mean, event_event_variance, event_nonevent_mean, event_nonevent_variance):\n",
    "    # calculate the probabilities of the new observation belonging to event/event and event/non-event distributions\n",
    "    prob_event_event = calculate_prob(new_observation, event_event_mean, event_event_variance)\n",
    "    prob_event_nonevent = calculate_prob(new_observation, event_nonevent_mean, event_nonevent_variance)\n",
    "    \n",
    "    # classify the new observation as belonging to the event class or non-event class based on the probabilities\n",
    "    if prob_event_event > prob_event_nonevent:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    # calculate the accuracy, precision, recall, and F1-Score\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    tp = np.sum((np.array(y_true) == 1) & (np.array(y_pred) == 1))+np.sum((np.array(y_true) == 0) & (np.array(y_pred) == 0))\n",
    "    fp = np.sum((np.array(y_true) == 0) & (np.array(y_pred) == 1))\n",
    "    fn = np.sum((np.array(y_true) == 1) & (np.array(y_pred) == 0))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testing_data, mid, event_index, nonevent_index, buffer, event_event_mean, event_event_variance, event_nonevent_mean, event_nonevent_variance):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(testing_data.shape[0]):\n",
    "        # get the true class label for each observation in the testing data\n",
    "        if i in event_index:\n",
    "            y_true.append(1)\n",
    "        else:\n",
    "            y_true.append(0)\n",
    "        \n",
    "        # compute the average Manhattan distance between the new observation and the event class vectors\n",
    "        # event_vectors = testing_data[i, event_index] # we should use training data here instead of test data because if we use test the we are also looking into the future. \n",
    "        avg_distance = np.sum(np.abs(testing_data[i] - buffer))/len(buffer) # This has to be different. \n",
    "        # print(avg_distance)\n",
    "        # classify the new observation\n",
    "        y_pred.append(classify(avg_distance, event_event_mean, event_event_variance, event_nonevent_mean, event_nonevent_variance))\n",
    "    \n",
    "    # evaluate the performance of the classification\n",
    "    accuracy, precision, recall, f1 = evaluate(y_true, y_pred)\n",
    "    class_imbalance = (len(event_index_test)/len(non_event_index_test))*100\n",
    "    print('The accuracy is:', accuracy)\n",
    "    print('The precision is:', precision)\n",
    "    print('The recall is:', recall)\n",
    "    print('The f1 is:', f1)\n",
    "    print('The class imbalance:', class_imbalance)\n",
    "    print('event number in test set:', len(event_index_test))\n",
    "    print('non event in test set:', len(non_event_index_test))\n",
    "    return accuracy, precision, recall, f1, class_imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = matrix1.shape[0] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = matrix1[:mid, :] #first half of the whole data to maintain the temporal lookback error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = matrix1[mid:, :] #second half of the whole data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_index_train = [num for num in index_list if num < mid] #get all the event index from the cvar index list until mid\n",
    "event_index_test = [num for num in index_list if num > mid] #get all the event index from the cvar index list until mid\n",
    "non_event_index_test = [num for num in range(mid, matrix1.shape[0] ) if num not in index_list] #all natural no not present in event index list until mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_index_test = [(x-mid) for x in event_index_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_event_index_test = [(x-mid) for x in non_event_index_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer1 = training_data[np.array(event_index_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true, y_pred = test(testing_data, mid, event_index_test, non_event_index_test, buffer1, event_event_mean, event_event_var, event_non_event_mean, event_non_event_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(testing_data, mid, event_index_test, non_event_index_test, buffer1, event_event_mean, event_event_var, event_non_event_mean, event_non_event_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
